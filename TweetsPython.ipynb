{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a424d1",
   "metadata": {},
   "source": [
    "# Covid Tweets Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef7f28",
   "metadata": {},
   "source": [
    "#### This script is used to prepare and clean a small, example sample of the Covid Tweets Dataset. The main idea of this script is to demonstrate some of the cleaning capabilities of the framework. For the experiment a sample of 1000 (rows) in English will be used. The dataset contains 9 errors types previously added from the \"Insert Dirty Data\" script and also contains human-generated text from the posts that will be prepared and cleaned through NLP (Natural Language Processing) tasks. The first section of the script is the NLP tasks, which are as follows: \n",
    "    1. Removing emojis. \n",
    "    2. Removing hyperlinks.\n",
    "    3. Removing new lines (\\n).\n",
    "    4. Removing HTML entities. \n",
    "    5. Removing hashtags (#) and tagged accounts (@).\n",
    "    6. Splitting the attached strings.\n",
    "    7. Lowercasing all words.\n",
    "    8. Transforming contractions.\n",
    "    9. Removing punctuations.\n",
    "    10. Transforming slang words.\n",
    "    11. Removing stop words.\n",
    "#### The second section of the script is cleaning the errors types in the other columns, which are as follows:\n",
    "    1. Cleaning wrong data type errors. \n",
    "    2. Cleaning negative number representation errors.\n",
    "    3. Cleaning extraneous data.\n",
    "    4. Swapping row ordering.\n",
    "    5. Placing data into correct fields.\n",
    "    6. Cleaning duplicated data.\n",
    "    7. Cleaning misspelled data.\n",
    "    8. Cleaning special character errors.\n",
    "    9. Cleaning inconsistent data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db7dd2",
   "metadata": {},
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a321b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For output cleaning\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Remove the comments in order to perform the necessary installations!\n",
    "\n",
    "#!pip install opencv-python\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#!pip install re\n",
    "#!pip install xlrd\n",
    "#!pip install openpyxl\n",
    "\n",
    "# Clear the output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556d476",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c25de715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries.\n",
    "import re \n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Clear the output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1cf9f",
   "metadata": {},
   "source": [
    "### 1. NLP processing of text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01b765",
   "metadata": {},
   "source": [
    "Download all stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e8ab805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Change dataframe column and row width limits\n",
    "pd.options.display.max_colwidth = 10000\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Clear the output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e1a9d",
   "metadata": {},
   "source": [
    "Load 1651 tweets into a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "025486b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exactly 1651 tweets are being loaded because 651 are not in English, and later in the code will be removed.\n",
    "# Leaving 1000 rows for testing.\n",
    "df = pd.read_csv('D:\\TweetCSV\\TweetsWorkbookCSV.csv', nrows=1651)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec295612",
   "metadata": {},
   "source": [
    "Remove all unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "51e6e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all other columns and focuses only on the \"text\" column.\n",
    "df.drop(['coordinates', 'created_at', 'hashtags', 'media', 'urls', 'in_reply_to_status_id', 'in_reply_to_user_id', \n",
    "         'user_followers_count', 'favorite_count', 'in_reply_to_screen_name', 'place', 'possibly_sensitive', \n",
    "         'quote_id', 'retweet_count', 'retweet_id', 'retweet_screen_name', 'user_created_at', 'user_id', \n",
    "         'user_default_profile_image', 'user_description', 'user_favourites_count', 'user_screen_name',\n",
    "         'user_statuses_count', 'user_time_zone', 'user_urls', 'user_verified', 'source', 'tweet_url', \n",
    "         'user_friends_count', 'user_listed_count', 'user_location', 'user_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7d5974",
   "metadata": {},
   "source": [
    "Remove all rows that are not in English. Leaving exactly 1000 rows in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "475756e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaves only the rows that are in english language.\n",
    "df.drop(df.index[df['lang'] != 'en'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f18571",
   "metadata": {},
   "source": [
    "#### Remove all emojis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "32152a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all emojis through the use of a lambda.\n",
    "filter_char = lambda c: ord(c) < 256\n",
    "df['text'] = df['text'].apply(lambda s: ''.join(filter(filter_char, s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e06bb",
   "metadata": {},
   "source": [
    "#### Remove all hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "25ac5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hyperlinks through a regex.\n",
    "df = df.replace('http[s]?:\\S+|www.\\S+', '',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f7954",
   "metadata": {},
   "source": [
    "#### Remove all new lines (\\n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5c574980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all new lines and replace them with empty space.\n",
    "df['text'] = df['text'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd17ae",
   "metadata": {},
   "source": [
    "#### Remove all HTML entities such as &apos, &amp and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "92e6c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all HTML.\n",
    "df = df.replace('&\\S+', '',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8766dd4",
   "metadata": {},
   "source": [
    "#### Remove all tagged accounts and all hashtag words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "772f8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all tagged accounts and remove all # signs, leaving the words. \n",
    "df = df.replace('@\\S+', '',regex=True)\n",
    "df = df.replace('#', '',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1fb23",
   "metadata": {},
   "source": [
    "#### Split all attached strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ca960096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all attached strings that could have been left from the hashtags or other places.\n",
    "for key, value in df['text'].iteritems():\n",
    "    df.loc[key, 'text'] = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\", value) if s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce80c5",
   "metadata": {},
   "source": [
    "#### Turn all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8e1157a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all words to lowercase through a lambda.\n",
    "df['text'] = df['text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a436c8",
   "metadata": {},
   "source": [
    "#### Transform all contractions to the actual words. Such as \"can't\" = \"cannot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7af276a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file with all contractions.\n",
    "file=open(\"D:\\Contractions.txt\",\"r\")\n",
    "contractions=file.read()\n",
    " \n",
    "# Separate each line from the file.\n",
    "contractions=contractions.split('\\n')\n",
    " \n",
    "contr=[]\n",
    "contr_meaning=[]\n",
    " \n",
    "# Store the contractions and their actual meanings in different lists.\n",
    "for line in contractions:\n",
    "    temp=line.split(\":\")\n",
    "    contr.append(temp[0])\n",
    "    contr_meaning.append(temp[-1])\n",
    "\n",
    "# Loop through all rows in the dataframe.\n",
    "for row in df['text'].iteritems():\n",
    "    wordList = row[1].split(\" \")\n",
    "    # Loop through all words in a row.\n",
    "    for word in wordList:\n",
    "        # Loop through all contractions.\n",
    "        for c in enumerate(contr):\n",
    "            # Compare each word with each contraction.\n",
    "            if word == c[1]:\n",
    "                index = wordList.index(word)\n",
    "                idx=c[0]\n",
    "                # Swap contraction with its meaning.\n",
    "                wordList[index] = contr_meaning[idx]\n",
    "        # Swap old row with new one in the dataframe. \n",
    "        df.loc[row[0], 'text'] = \" \".join(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a33794",
   "metadata": {},
   "source": [
    "#### Remove all punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f3f2331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations through the use of a regex.\n",
    "df = df.replace('[^\\w\\s]', '',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e33207",
   "metadata": {},
   "source": [
    "Turn the text into arrays. For better processing of stop words and slang words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "21bdd587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DF object to temporarily store transformed rows.\n",
    "temp_df = pd.DataFrame(columns=['TextArrays'])\n",
    "\n",
    "# Loop through all rows in the dataframe.\n",
    "for key, value in df['text'].iteritems():\n",
    "    temp_list = value.split()\n",
    "    # Remove all empty or spaced objects in the list.\n",
    "    if '' in temp_list:\n",
    "        temp_list.remove('')\n",
    "    if ' ' in temp_list:\n",
    "        temp_list.remove(' ')\n",
    "    # Append the lists to the temporary dataframe.\n",
    "    temp_df = temp_df.append({'TextArrays': temp_list}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e3e4d",
   "metadata": {},
   "source": [
    "#### Turning slang words to actual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "75bc82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file with all slang words.\n",
    "file=open(\"D:\\SlangWordsMeaning.txt\",\"r\")\n",
    "slangWords=file.read()\n",
    " \n",
    "# Separate each line from the file.\n",
    "slangWords=slangWords.split('\\n')\n",
    " \n",
    "slang_word=[]\n",
    "meaning=[]\n",
    " \n",
    "# Store the slang words and their actual meanings in different lists.\n",
    "for line in slangWords:\n",
    "    temp=line.split(\":\")\n",
    "    slang_word.append(temp[0])\n",
    "    meaning.append(temp[-1])\n",
    "\n",
    "# Create a new DF object to store transformed rows.\n",
    "temp_df5 = pd.DataFrame(columns=['Final'])\n",
    "\n",
    "# Loop through all rows in the dataframe.\n",
    "for row in temp_df['TextArrays'].iteritems():\n",
    "    # Loop through all words in a row.\n",
    "    for word in row[1]:\n",
    "        # Loop through all slang words.\n",
    "        for slang in enumerate(slang_word):\n",
    "            # Compare each word with each slang word.\n",
    "            if word == slang[1]:\n",
    "                index = row[1].index(word)\n",
    "                idx=slang[0]\n",
    "                # Remove previous slang word from list.\n",
    "                del row[1][index]\n",
    "                # Append the meaning of the slang word as different list objects.\n",
    "                for x in meaning[idx].split():\n",
    "                  row[1].append(x)\n",
    "    # Add the transformed rows to the new dataframe.\n",
    "    temp_df5 = temp_df5.append({'Final': row[1]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace022c6",
   "metadata": {},
   "source": [
    "#### Remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fc3678a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DF object to store transformed rows.\n",
    "temp_df2 = pd.DataFrame(columns=['Final'])\n",
    "\n",
    "# Loop through all rows in the dataframe.\n",
    "for key, value in temp_df5['Final'].iteritems():\n",
    "    # Iter a copy of each row (used to prevent bugs in removal).\n",
    "    for word in list(value):\n",
    "        # If a word is a stop word, remove it.\n",
    "        if word in stopwords.words('english'):\n",
    "            value.remove(word)\n",
    "    # Add the transformed rows to the new dataframe.    \n",
    "    temp_df2 = temp_df2.append({'Final': value}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4284da79",
   "metadata": {},
   "source": [
    "Save the arrays of words to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eb131b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "temp_df2.to_excel(\"D:\\Text_After_NLP.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29f2f1",
   "metadata": {},
   "source": [
    "### 2. Cleaning dirty data from the Tweets Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca27c85",
   "metadata": {},
   "source": [
    "Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0d67403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dirty file\n",
    "df2 = pd.read_excel(\"D:\\DirtyTweetsDataset.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc846c",
   "metadata": {},
   "source": [
    "Remove redundant columns that have been created when saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "36c23bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns 'user_default_profile_image'] (if there are any)\n",
    "df2.drop(['Unnamed: 0', 'Unnamed: 0.1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36617efa",
   "metadata": {},
   "source": [
    "#### Cleaning the inconsistent data in the 'lang' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dfb4d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "empt_dict = {}\n",
    "\n",
    "# Through the use of a dictionary get each row and its number of occurances \n",
    "for index, row in df2.iterrows():\n",
    "    l = row['lang']\n",
    "    if l in empt_dict:\n",
    "        empt_dict[l] += 1\n",
    "    else: \n",
    "        empt_dict[l] = 1\n",
    "\n",
    "# Get the most frequent row \n",
    "most_frequent_key = max(empt_dict, key=empt_dict.get)\n",
    "\n",
    "# Update all rows to be the same as the most frequent\n",
    "for index, row in df2.iterrows():\n",
    "    if row['lang'] == most_frequent_key:\n",
    "        continue\n",
    "    else:\n",
    "        df2.loc[index, 'lang'] = most_frequent_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd3f81",
   "metadata": {},
   "source": [
    "#### Cleaning the wrong data type errors in the 'user_default_profile_image' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9b85b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "empt_dict2 = {}\n",
    "\n",
    "# Function to get similarity between elements\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "# Through the use of a dictionary get type of row and its number of occurances \n",
    "for index, row in df2.iterrows():\n",
    "    final = \"\"\n",
    "    # First find the type of the row\n",
    "    # String\n",
    "    if type(row['user_default_profile_image']) is str:\n",
    "        final = str\n",
    "    # Integer \n",
    "    elif type(row['user_default_profile_image']) is int:\n",
    "        final = int\n",
    "    # Boolean\n",
    "    elif type(row['user_default_profile_image']) is bool:\n",
    "        final = bool\n",
    "    \n",
    "    # Increment accordingly \n",
    "    if final in empt_dict2:\n",
    "        empt_dict2[final] += 1\n",
    "    else: \n",
    "        empt_dict2[final] = 1\n",
    "\n",
    "# Get the most frequent type\n",
    "most_frequent_key2 = max(empt_dict2, key=empt_dict2.get)\n",
    "\n",
    "# Update the rows accordingly \n",
    "for index, row in df2.iterrows():\n",
    "    if type(row['user_default_profile_image']) == most_frequent_key2:\n",
    "        continue\n",
    "    else:\n",
    "        # Through the use of similarity decide whether to update to true or false\n",
    "        if similar(row['user_default_profile_image'], 'true') > similar(row['user_default_profile_image'], 'false'):\n",
    "            df2.loc[index, 'user_default_profile_image'] = True\n",
    "        else:\n",
    "            df2.loc[index, 'user_default_profile_image'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed43410",
   "metadata": {},
   "source": [
    "#### Cleaning the special characters in the 'user_favourites_count' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b86ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all rows\n",
    "for index, row in df2.iterrows():\n",
    "    # Check if row is string (since integer rows can't have special numbers)\n",
    "    if type(row['user_favourites_count']) == str:\n",
    "        # Check if row has special characters\n",
    "        if not row['user_favourites_count'].isalnum():\n",
    "            # Update row by removing the special character and casting it to int\n",
    "           df2.loc[index, 'user_favourites_count'] = int(''.join(s for s in row['user_favourites_count'] if s.isalnum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4cd811",
   "metadata": {},
   "source": [
    "#### Cleaning the negative number representation in the 'retweet_count' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ef9c2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all rows\n",
    "for index, row in df2.iterrows():\n",
    "    # Check if number is negative\n",
    "    if row['retweet_count'] < 0:\n",
    "        # If num is -1 then it was converted from 0\n",
    "        if row['retweet_count'] == -1:\n",
    "            df2.loc[index, 'retweet_count'] = 0\n",
    "        # Make positive\n",
    "        else: \n",
    "             df2.loc[index, 'retweet_count'] = abs(row['retweet_count'])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96a4be",
   "metadata": {},
   "source": [
    "#### Cleaning the extraoneous data in the 'user_created_at' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b126ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates = []\n",
    "\n",
    "# Assume that the extra value is at the begining of the row and is from the user_id since there are blank rows\n",
    "# Loop through the rows\n",
    "for index, row in df2.iterrows():\n",
    "    arr = row['user_created_at'].split(' ')\n",
    "    # If arr is longer than usual \n",
    "    if len(arr) > 6:\n",
    "        list_dates.append(float(arr[0]))\n",
    "        arr.pop(0)\n",
    "        df2.loc[index, 'user_created_at'] =  ' '.join(arr)\n",
    "\n",
    "# Loop to bring back the id's to their column\n",
    "for index, row in df2.iterrows():\n",
    "    # If row is NaN\n",
    "    if math.isnan(row['user_id']):\n",
    "        # If list not empty\n",
    "        if list_dates:\n",
    "            # Assign and remove\n",
    "            df2.loc[index, 'user_id'] = list_dates[0]\n",
    "            del list_dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac477a33",
   "metadata": {},
   "source": [
    "#### Cleaning the misspelled data in the 'user_created_at' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d1bda467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all rows\n",
    "for index, row in df2.iterrows():\n",
    "    arr = row['user_created_at'].split(' ')\n",
    "    empt_dict = {}\n",
    "    \n",
    "    # Get the similarities \n",
    "    empt_dict['Mon'] = similar(arr[0], 'Mon')\n",
    "    empt_dict['Tue'] = similar(arr[0], 'Tue')\n",
    "    empt_dict['Wed'] = similar(arr[0], 'Wed')\n",
    "    empt_dict['Thu'] = similar(arr[0], 'Thu')\n",
    "    empt_dict['Fri'] = similar(arr[0], 'Fri')\n",
    "    empt_dict['Sat'] = similar(arr[0], 'Sat')\n",
    "    empt_dict['Sun'] = similar(arr[0], 'Sun')\n",
    "    \n",
    "    # Get the most similar day of the week\n",
    "    most_similar_key = max(empt_dict, key=empt_dict.get)\n",
    "    \n",
    "    # Assign the new day of the week\n",
    "    arr[0] = most_similar_key\n",
    "    df2.loc[index, 'user_created_at'] =  ' '.join(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0a7a5",
   "metadata": {},
   "source": [
    "#### Cleaning the wrongly inserted data in the 'source' and 'created_at' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f167f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "empt_dict = {}\n",
    "\n",
    "# Loop through all rows\n",
    "# Assume that the odd rows must go to the 'source' column \n",
    "for index, row in df2.iterrows():\n",
    "    r = row['source']\n",
    "    if r in empt_dict:\n",
    "        empt_dict[r] += 1\n",
    "    else: \n",
    "        empt_dict[r] = 1\n",
    "    \n",
    "# Get the most frequent row \n",
    "most_frequent_key = max(empt_dict, key=empt_dict.get)\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    sim = similar(most_frequent_key, row['source'])\n",
    "    f = float(\"{:.2f}\".format(sim))\n",
    "    if f < 0.30:\n",
    "        temp = df2.loc[index, 'source']\n",
    "        df2.loc[index, 'source'] = df2.loc[index, 'created_at']\n",
    "        df2.loc[index, 'created_at'] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d89df",
   "metadata": {},
   "source": [
    "#### Cleaning the duplicate data in the 'user_id' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f13711e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It has been decided that the framework will deal with duplicates by eliminating them. \n",
    "# Pandas provides a drop_duplicates function which will remove all of the duplicates in the dataframe column.\n",
    "# 'keep' will leave the first occurance of the 'user_id'.\n",
    "df2.drop_duplicates(subset =\"user_id\",\n",
    "                     keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7545e5",
   "metadata": {},
   "source": [
    "#### Fixing the swapped orderings in the 'created_at' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "60632b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all rows\n",
    "for index, row in df2.iterrows():\n",
    "    # Check if row starts with a String, if not then assume its dirty\n",
    "    if re.search(\"^[a-zA-Z]\", row['created_at']) is not None:\n",
    "        continue\n",
    "    # If dirty then clean it\n",
    "    else:\n",
    "        arr = row['created_at'].split(' ')\n",
    "        # Loop through all elements of the row\n",
    "        for a in arr:\n",
    "            if re.search(\"^[a-zA-Z]\", a) is not None:\n",
    "                break\n",
    "            # Append to the end of the array, to fix order\n",
    "            else:\n",
    "                arr.append(a)                \n",
    "                \n",
    "        # Delete the redundant elements\n",
    "        if len(arr) > 6:\n",
    "            del arr[0:3]\n",
    "                  \n",
    "        # Update the rows\n",
    "        df2.loc[index, 'created_at'] =  ' '.join(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336196c1",
   "metadata": {},
   "source": [
    "Save the cleaned tweets to an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1fc11783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df2.to_excel(\"D:\\Cleaned_English_Tweets.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
